{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data:\n",
      "         content_id                                            content  \\\n",
      "0  vUXizsqexyZVRdFH           因为森林人即将换代，这套系统没必要装在一款即将换代的车型上，因为肯定会影响价格。   \n",
      "1  4QroPd9hNfnCHVt7      四驱价格貌似挺高的，高的可以看齐XC60了，看实车前脸有点违和感。不过大众的车应该不会差。   \n",
      "2  QmqJ2AvM5GplaRyz  斯柯达要说质量，似乎比大众要好一点，价格也低一些，用料完全一样。我听说过野帝，但没听说过你说...   \n",
      "3  KMT1gFJiU4NWrVDn           这玩意都是给有钱任性又不懂车的土豪用的，这价格换一次我妹夫EP020可以换三锅了   \n",
      "4  nVIlGd5yMmc37t1o                            17价格忒高，估计也就是14-15左右。      \n",
      "\n",
      "  subject  sentiment_value sentiment_word  \n",
      "0      价格                0             影响  \n",
      "1      价格               -1              高  \n",
      "2      价格                1              低  \n",
      "3      价格               -1           有钱任性  \n",
      "4      价格               -1              高  \n",
      "         content_id                          content\n",
      "0  XuPwKCnA2fqNh5vm             欧蓝德，价格便宜，森林人太贵啦！    \n",
      "1  2jNbDn85goX3IuPE                楼主什么时候提的车，南昌优惠多少啊\n",
      "2  hLgEADQ8sUnvGFK9         吉林，2.5优惠20000，送三年九次保养，贴膜\n",
      "3  nZmM7LQsfr03wUaz     便宜2万的豪华特装，实用配制提升，优惠还给力，确实划算。\n",
      "4  pwd8MnrthDqLZafe  如果实在想买就等车展期间，优惠2万，我24.98万入的2.5豪\n",
      "analyse data:\n",
      "{'价格': 0.1279782849100231, '配置': 0.08575449884387253, '操控': 0.10415200562983815, '舒适性': 0.09359605911330049, '油耗': 0.1087765155323213, '动力': 0.27465567507791294, '内饰': 0.05388559364632552, '安全性': 0.05760530813310546, '空间': 0.044435508193425156, '外观': 0.04916055091987534}\n",
      "{0: 0.6696491404443551, -1: 0.16246104353071278, 1: 0.16788981602493214}\n",
      "pretreatment:\n",
      "         content_id                                            content  \\\n",
      "0  vUXizsqexyZVRdFH              森林 人 换代 这套 系统 必要 装在 一款 换代 车型 肯定 影响 价格   \n",
      "1  4QroPd9hNfnCHVt7          四驱 价格 貌似 挺 高 高 看齐 实车 前 脸 有点 违和感 大众 车 应该 差   \n",
      "2  QmqJ2AvM5GplaRyz  斯柯达 要说 质量 似乎 大众 好 一点 价格 低 一些 用料 完全 听说 过野帝 没听说过...   \n",
      "3  KMT1gFJiU4NWrVDn                     玩意 有钱 任性 懂车 土豪 价格 换 一次 妹夫 换 三锅   \n",
      "4  nVIlGd5yMmc37t1o                                       价格 忒 高 估计 左右   \n",
      "\n",
      "  subject  sentiment_value sentiment_word  \n",
      "0      价格                0             影响  \n",
      "1      价格               -1              高  \n",
      "2      价格                1              低  \n",
      "3      价格               -1           有钱任性  \n",
      "4      价格               -1              高  \n",
      "         content_id                            content\n",
      "0  XuPwKCnA2fqNh5vm                   欧蓝德 价格便宜 森林 人 太贵\n",
      "1  2jNbDn85goX3IuPE                       楼主 提 车 南昌 优惠\n",
      "2  hLgEADQ8sUnvGFK9                吉林 优惠 送 三年 九次 保养 贴膜\n",
      "3  nZmM7LQsfr03wUaz  便宜 万 豪华 特装 实用 配制 提升 优惠 还给 力 确实 划算\n",
      "4  pwd8MnrthDqLZafe             实在 想 买 车展 期间 优惠 万 万入 豪\n",
      "train subject:\n",
      "SVM correct prediction: 0.7010\n",
      "train sentiment_value:\n",
      "SVM correct prediction: 0.6739\n",
      "output\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "#import keras\n",
    "#import gensim\n",
    "import jieba\n",
    "import pandas as pd\n",
    "#import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#统计频率\n",
    "def count(list):\n",
    "    words = {}\n",
    "    for w in list:\n",
    "        if words.get(w):\n",
    "            words[w] += 1\n",
    "        else:\n",
    "            words[w] = 1\n",
    "    return words\n",
    "\n",
    "#计算百分比\n",
    "def percentage(data):\n",
    "    s = np.sum(list(data.values()))\n",
    "    res = {}\n",
    "    for i in data.keys():\n",
    "        res[i] = np.divide(data[i], s)\n",
    "    return res\n",
    "    \n",
    "#分析训练数据\n",
    "def analyse_traindata(train_data):\n",
    "    print(percentage(count(train_data[\"subject\"])))\n",
    "    print(percentage(count(train_data[\"sentiment_value\"])))\n",
    "    #print(count(train_data[\"sentiment_word\"]))\n",
    "    return 0\n",
    "\n",
    "#a=\"因为森林人即将换代，这套系统没必要装在一款即将换代的车型上，因为肯定会影响价格\"\n",
    "\n",
    "#只保留中文    \n",
    "def chinese_remained(line): \n",
    "    rule=re.compile(\"[^\\u4e00-\\u9fa5]\")\n",
    "    line=re.sub(rule,\"\",line)\n",
    "    return line\n",
    "#b=chinese_remained(a)\n",
    "#print(b)\n",
    "\n",
    "#分词 直接用结巴分词\n",
    "def participle(line):\n",
    "    words = jieba.cut(line,cut_all=False)    \n",
    "    res = ''\n",
    "    for w in words:\n",
    "        if w != '\\t':\n",
    "            res += w + \" \"\n",
    "    return res.strip()\n",
    "#c=participle(b)\n",
    "#print(c)  \n",
    "\n",
    "#删除停用词 停用词库的选择？\n",
    "def delete_stopwords(line,stop_words_source):\n",
    "    stopwords = [w.strip() for w in codecs.open(stop_words_source, 'r', encoding='utf-8').readlines()]\n",
    "    words = line.split(' ')          \n",
    "    res = ''\n",
    "    for w in words:\n",
    "        w = w.strip()\n",
    "        if w not in stopwords:\n",
    "            if w != '\\t':\n",
    "                res += w + \" \"\n",
    "    return res.strip()\n",
    "#d=delete_stopwords(c,stop_words_source)\n",
    "#print(d)\n",
    "\n",
    "#数据预处理 主要处理content\n",
    "def pretreatment(train_data):\n",
    "    num_data=train_data.shape[0]\n",
    "    for i in range(num_data):\n",
    "        tmp=delete_stopwords(participle(chinese_remained(train_data['content'][i])),stop_words_source)\n",
    "        train_data.loc[i, \"content\"] = tmp\n",
    "        #print(tmp)\n",
    "    return train_data\n",
    "\n",
    "#训练两个多分类器 ？sentiment_word不用管了\n",
    "#svm\n",
    "def classification(x_train,x_validation, y_train, y_validation,test_data):\n",
    "    svm = Pipeline([('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, max_iter= 5)),\n",
    "    ])\n",
    "    svm.fit(x_train, y_train)  \n",
    "    validation_predicted = svm.predict(x_validation)\n",
    "    print('SVM correct prediction: {:4.4f}'.format(np.mean(validation_predicted == y_validation)))\n",
    "    test_predicted=svm.predict(test_data.values)\n",
    "    return test_predicted\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    stop_words_source='./stopWord.txt'\n",
    "    #subject_type=[\"动力\",\"价格\",\"内饰\",\"配置\",\"安全性\",\"外观\",\"操控\",\"油耗\",\"空间\",\"舒适性\"]    \n",
    "    train_data=pd.read_csv('./train.csv',encoding='utf-8')\n",
    "    test_data=pd.read_csv('./test_public.csv',encoding='utf-8')\n",
    "    #计算缺失值\n",
    "    #train_data[train_data.isnull().values==True]\n",
    "    #train_data.shape[0] - train_data.count()\n",
    "    #只缺失sentiment_word     7778\n",
    "    print(\"load data:\")\n",
    "    print(train_data.head())\n",
    "    print(test_data.head())\n",
    "    #print(train_data['content'])    \n",
    "    print(\"analyse data:\")\n",
    "    analyse_traindata(train_data)\n",
    "    print(\"pretreatment:\")\n",
    "    train_data_p=pretreatment(train_data)\n",
    "    print(train_data_p.head())\n",
    "    test_data_p=pretreatment(test_data)\n",
    "    print(test_data_p.head())\n",
    "    #划分训练集合测试集 抽五分之一作为验证集\n",
    "    xsub_train, xsub_validation, ysub_train, ysub_validation = train_test_split(train_data_p['content'].values, train_data_p['subject'].values, test_size=0.2)\n",
    "    xsen_train, xsen_validation, ysen_train, ysen_validation = train_test_split(train_data_p['content'].values, train_data_p['sentiment_value'].values, test_size=0.2)\n",
    "    #进行训练\n",
    "    print(\"train subject:\")\n",
    "    subject_predicted=classification(xsub_train, xsub_validation, ysub_train, ysub_validation,test_data['content'])\n",
    "    print(\"train sentiment_value:\")\n",
    "    sentiment_value_predicted=classification(xsen_train, xsen_validation, ysen_train, ysen_validation,test_data['content'])\n",
    "    #输出结果到csv中\n",
    "    print(\"output\")\n",
    "    with codecs.open('./output.csv', \"w\", \"utf-8\") as outfile:\n",
    "        outfile.write(\"content_id,subject,sentiment_value,sentiment_word\\n\")\n",
    "        cnt = 1\n",
    "        for content_id, subject, sentiment_value in zip(test_data[\"content_id\"], subject_predicted, sentiment_value_predicted):\n",
    "            outfile.write(\"{},{},{},\\n\".format(content_id, subject, sentiment_value))\n",
    "            cnt += 1    \n",
    "    print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
